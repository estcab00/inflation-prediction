#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
%2multibyte Version: 5.50.0.2890 CodePage: 932
\usepackage{amsfonts}
\usepackage{lscape}
\usepackage{graphicx}
\usepackage{accents}
\usepackage{setspace}
\setcounter{MaxMatrixCols}{30}
%TCIDATA{OutputFilter=latex2.dll}
%TCIDATA{Version=5.50.0.2890}
%TCIDATA{Codepage=932}
%TCIDATA{CSTFile=article.cst}
%TCIDATA{Created=Sunday, August 23, 2015 23:07:57}
%TCIDATA{LastRevised=Wednesday, August 16, 2023 19:20:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{<META NAME="DocumentShell" CONTENT="Standard LaTeX\Standard LaTeX Article">}
%TCIDATA{Language=American English}
%TCIDATA{PageSetup=43,43,43,43,0}
%TCIDATA{Counters=arabic,1}
%TCIDATA{AllPages=
%H=36
%F=36
%}
%BeginMSIPreambleData
\providecommand{\U}[1]{\protect \rule{.1in}{.1in}}
%EndMSIPreambleData
\topmargin -1pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 9in
\oddsidemargin -0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in
\let \large=\normalsize
\let \Large=\large
\let \tilde=\widetilde
\newtheorem{theorem}{Theorem}\newtheorem{Assumption}{Assumption}\newtheorem{lemma}{Lemma}\newtheorem{problem}{Property}\newtheorem{proposition}{Proposition}\newtheorem{corollary}{Corollary}\renewcommand{\arraystretch}{1.25}
\newenvironment{proof}[1][P
roof]{\textbf{#1.} }{\  \rule{0.5em}{0.5em}}


\setlength{\tabcolsep}{1.75pt}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package none
\inputencoding auto
\fontencoding default
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref true
\pdf_bookmarks false
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref section
\pdf_pdfusetitle false
\pdf_quoted_options "linkcolor=blue,filecolor=blue,citecolor=black,urlcolor=blue"
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 0
\use_package cancel 0
\use_package esint 1
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\paragraph_spacing onehalf
Assessing forecasting in linear and non-linear Machine Learning models for
 short-term inflation in Peru
\end_layout

\begin_layout Author
Esteban Cabrera Bonilla
\begin_inset Foot
status open

\begin_layout Plain Layout
E-Mail Address: esteban.cabrera@pucp.edu.pe.
\end_layout

\end_inset

 
\begin_inset Newline newline
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout

%EndAName
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset

Pontificia Universidad Católica del Perú
\end_layout

\begin_layout Date
This Version: March 26, 2024
\end_layout

\begin_layout Abstract
\noindent
Inflation forecasting is a key element of monetary policy.
 Bayesian techniques have been used to improve upon univariate methods.
 A relatively recent approach to forecasting has been the implementation
 of Machine Learning methods.
 In this document we compare two types of methods, benchmark models, commonly
 used to predict future inflation, with linear and non-linear Machine Learning
 methods for inflation forecasting in Peru after the implementation of an
 inflation-targeting regime.
 Our goal is to demonstrate that both linear and non-linear machine learning
 models can easily outperform traditional models, and even Bayesian techniques
 in a in an emerging economy with a steady price level like Peru.
 We will also find out if non-linear models are most suitable for inflation
 forecasting in Peru, which would imply the presence of non-linear relationships
 between inflation and its predictors.
\end_layout

\begin_layout Abstract
\noindent
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Abstract
\noindent

\series bold
JEL Classification : 
\series default
E31, C49, C53, C59.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Abstract
\noindent

\series bold
Keywords:
\series default
 Inflation, forecasting, Machine Learning, Peru.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
thispagestyle{empty}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{page}{0}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
baselineskip
\end_layout

\end_inset

=14.4pt
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
As Stock and Watson (2001) said, macroeconometricians are given four main
 tasks: “describe and summarize macroeconomic data, make macroeconomic forecasts
, quantify what we do or do not know about the true structure of the macroeconom
y, and advise macroeconomic policymakers.” In this thesis, we will focus
 on the second and fourth tasks, macroeconomic forecasts and policy advice.
 Inflation is one of the most important macroeconomic indicators for a country,
 therefore it is crucial to keep it monitored.
 Across the world, one thing that most central banks have in common is the
 goal to keep the steadiness of price levels.
 In Peru, for instance, the main purpose of the Central Bank of Reserve
 (BCRP) is “to preserve monetary stability” (BCRP, n/d), which translates
 into keeping inflation under control.
 Similarly, monetary authorities of other countries in the region pursue
 the same objective.
\end_layout

\begin_layout Standard
Inflation forecasting is key to dictating an accurate monetary policy.
 Due to the lag between implementing monetary policy and its effects, most
 central banks implement a forecast-based monetary policy.
 In that sense, inflation forecasting becomes one of the major tasks of
 policymakers since it is one of the main indicators that will drive monetary
 policy, the primary tool to target inflation.
 Forecasting also contributes to macroeconomic stabilization, as it helps
 private agents make better and more informed decisions (Carnot et al., 2011).
 
\end_layout

\begin_layout Standard
In 2001, the BCRP adopted the reference interest rate as its main policy
 instrument.
 The following year, 2002, it implemented an inflation targeting system
 to make the monetary policy decisions more transparent.
 These two changes, alongside other measures, such as the accumulation of
 large foreign exchange reserves on a large scale, changed the dynamics
 of inflation in Peru and allowed the central bank to ensured monetary stability
 in Peru in favourable and unfavourable external contexts over the first
 decades of the 21st century (Dancourt, 2014).
 When it comes to forecasting, these changes have made Peruvian inflation
 easier to forecast as it has become more stable since volatility has decreased.
 
\end_layout

\begin_layout Standard
The BCRP uses the dynamic semi-structural model known as Modelo de Proyección
 Trimestral (MPT) as its main forecasting and policymakingQuisque a enim
 blandit, tristique libero ac, dignissim nibh.
 In pellentesque leo eget placerat dapibus.
 Vestibulum iaculis, enim sodales aliquet pretium, mi mi feugiat diam, eu
 dignissim ante ipsum dignissim eros.
 Nullam et mollis lacus, sed fermentum enim.
 Duis tempus condimentum faucibus.
 Sed sodales placerat orci, vel fringilla nunc porta id.
 Ut vitae fermentum enim.
 Nam ultricies tempor velit, in convallis dolor dictum ac.
 tool.
 However, it has been pointed out that the model lacks parsimony, as it
 is composed by hundreds of equations.
 Llosa et al.
 (2006) proposes, alternatively, the use of Bayesian techniques and a small
 number of variables to accurately forecast Peruvian inflation.
 Barrera (2007) shows a robust sparse model approach to achieve parsimony,
 as these models identify the most relevant variables and discard the rest,
 reducing the number of variables used in the forecast.
\end_layout

\begin_layout Standard
Machine Learning (ML) is a relatively new approach to inflation forecasting.
 Therefore, there is little literature on the subject compared to other
 forecasting methods.
 Nevertheless, many of the papers on ML have concluded that both linear
 and non-linear multivariate ML models outperform other forecasting methods
 (Ülke et al., 2018; Medeiros et al., 2022).
 In particular, the find out that non-linear models, like Random Forest,
 are way better at predicting inflation than other models.
\end_layout

\begin_layout Standard
Across the region, there have been articles that have compared traditional
 univariate and multivariate econometric models with ML methods in terms
 of forecasting (Rodriguez- Vargas, 2020; Silva & Piazza, 2020).
 However, there has not been a proper comparison against more potent econometric
 methods, like Bayesian Vector Autoregressions.
 Furthermore, such an analysis with ML models has not yet been done for
 Peru.
\end_layout

\begin_layout Standard
Therefore, the purpose of this thesis is to evaluate the performance of
 different linear and non-linear univariate and multivariate models in terms
 of forecasting inflation for Peru after the introduction of the explicit
 inflation-targeting regime.
 Our benchmark econometric model will be a Random Walk (RW).
 We will compare econometric models such as Autoregressive Integrated Moving
 Average (ARIMA) and Vector Autoregression (VAR), while the ML methods will
 be a LASSO regression, Ridge regression, Elastic Net (EN) and Random Forest
 (RF).
 We will see which models perform better during three main forecasting periods:
 January 2009 - December 2009, January 2019 - December 2019 and January
 2023 - December 2023.
 We will see which variables contribute more to making better predictions.
 We will also find out if non-linear models are most suitable for inflation
 forecasting in Peru, which would imply the presence of non-linear relationships
 between inflation and its predictors.
\end_layout

\begin_layout Standard
The structure of the document includes the following: Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Second-section"
plural "false"
caps "false"
noprefix "false"

\end_inset

 outlines both the theoretical and empirical literature regarding inflation
 forecasting; Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Third-section"
plural "false"
caps "false"
noprefix "false"

\end_inset

 describes the theoretical framework that is going to be used to compare
 the forecasting models; Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Forth-section"
plural "false"
caps "false"
noprefix "false"

\end_inset

 portrays the data and describes the models; Section 5 comprises the discussion
 of the main results, followed by the concluding section.
\end_layout

\begin_layout Standard
Vivamus lobortis convallis hendrerit.
 Aliquam pharetra luctus nisl, quis sollicitudin ante fermentum et.
 Nunc efficitur enim id urna posuere vehicula.
 Nullam dui diam, ultrices vel enim non, imperdiet sagittis lectus.
 Curabitur elementum faucibus cursus.
 Donec eget posuere sapien, dapibus sagittis nulla.
 Etiam bibendum, tellus sit amet mollis egestas, urna eros egestas quam,
 a sagittis tellus purus id ante.
 Praesent vitae facilisis ante.
 
\begin_inset Foot
status open

\begin_layout Plain Layout
This is a footnote.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
And this is a citation 
\begin_inset CommandInset citation
LatexCommand citet
key "surname2020placeholder"
literal "true"

\end_inset

.
 You can reference to different sections in your paper, like this: See Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Third-section"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Section
Literature review
\begin_inset CommandInset label
LatexCommand label
name "sec:Second-section"

\end_inset


\end_layout

\begin_layout Standard
In this section we will review the main literature regarding inflation and
 time-series forecasting.
 Subsection 2.1 contains a brief describe of the evolution of forecasting
 theory and forecasting evaluation criteria.
 Subsection 2.2 examines different forecasting methods that has been used
 in empirical works, from simple univariate ARMA models, to State-Space
 representations, Stochastic Volatility models and Bayesian Vector Autoregressio
ns.
 It first shows international documentation, then regional literature followed
 by papers centred in Peruvian inflation.
 It then reviews the most recent method: Machine Learning models, and how
 they have been used to forecast inflation in countries in the region.
\end_layout

\begin_layout Subsection
Theorical Literature
\end_layout

\begin_layout Standard
The theory of economic forecasting has roots in the beginning of the twentieth
 century, with the first extensive work being that of Morgenstern (1928).
 According to Clements and Hendry (1998), based on reviews by Marget (1929),
 his work was the first one to discuss in-depth economic and business forecastin
g.
 He argued against it since economic data was not suitable for forecasting
 (it was not homogenous, nor independently distributed, and samples were
 too small to be used) and forecasting itself could impact agents’ reactions
 to them, making it ineffective (this was some sort of first ‘Lucas’s critique’).
\end_layout

\begin_layout Standard
Marget, on the other hand, while agreeing with the first argument of Morgenstern
, stated that forecasting should be viable as long as it was based on the
 extrapolation of previous patterns, rather than on probability-based techniques.
 This idea would imply that causation means predictability, which is not
 always the case.
 Marget would also support the idea of testing economic theories using forecasti
ng.
 Tinbergen (1939) was one of the first to do so, by constructing forecasting
 tests for econometric models developed for two reports delegated by the
 League of Nations.
 The role of econometric modelling and forecasting was later enhanced by
 the work of Theil (1961), Heesterman (1970), and Pindyck & Rubinfeld (1981),
 among others.
\end_layout

\begin_layout Standard
Another endeavour worth acknowledging is Haavelmo (1944).
 Haavelmo had a probabilistic approach to econometrics, proclaiming that
 the nature of economic data shouldn’t be an obstacle to applying statistical
 tools to its analysis.
 His forecasting approach (with a probabilistic framework) was the precursor
 of the textbook economic forecasting (Clements & Hendry, 1998).
 It is worth noting that his analysis required certain regularity in the
 series being forecasted (e.g., no structural breaks) since alternative forecastin
g techniques to deal with breaks in the data were yet to be developed.
\end_layout

\begin_layout Standard
The forecast evaluation criteria changed and developed during this period.
 The work of Klein (1950), Brown (1954) and Theil (1961) had a predominant
 role in the development of forecasting theory in the post-war period.
 Brown derived standard errors for forecasts from systems, while Theil developed
 the Theil index, a comparison measure for forecasting.
 Granger and Newbold (1973) showed that forecast accuracy cannot be based
 on comparisons of the predicted and actual time series or their distributional
 properties.
 Instead, we should focus on their difference, the forecast error.
 Ericsson (1992) considers the traditional use of the mean square forecast
 error (MSFE) in forecast evaluation, as well as the consistency of the
 models being compared.
\end_layout

\begin_layout Subsection
Empirical Literature
\end_layout

\begin_layout Standard
Based on the framework developed by Box and Jenkins (1976), most literature
 on forecasting in the post-war period focused on ARMA and ARIMA models
 (Anderson, 1977; Harvey & Todd, 1983).
 The Wold decomposition theorem (Wold, 1938) postulates that a purely determinis
tic covariance stationary process can have an infinite moving-average (MA)
 representation.
 Furthermore, any MA(∞) can be represented in accuracy by an ARMA(p,q) model.
 Due to the stochastic nature of most economic time series, they require
 differencing at least once in order to be stationary.
 For this reason, the integrated autoregressive moving average (ARIMA(p,d,q))
 models, in which the d indicates the number of times the series has been
 differentiated to be stationary, became dominant in modelling and forecasting.
\end_layout

\begin_layout Standard
Time series have also taken the form proposed by Kalman (1960), who designed
 a technique to identify and extract latent factors in state-space models.
 These models are useful since they relax the assumption that the explanatory
 variable is observable, which can help to explain the behaviour of some
 economic and financial time series.
 This allows for an easy way to model dynamic multivariate time series and
 avoids the need for proxy variables (Martin et al., 2013).
 The unobserved components approach has been proven to be a good fit for
 modelling and forecasting, competing and outperforming ARIMA models (Harvey
 & Todd, 1983; Stock & Watson, 1988, 1989, 1993, 2003).
 However, it has also been pointed out that when using UC models, there
 can be issues both in the specification of the models and in the inferences
 drawn from the distribution of the unobserved components (Maravelli, 1994).
\end_layout

\begin_layout Standard
A more recent approach of the unobserved components model with stochastic
 volatility (UC-SV) has been used to forecast inflation (Stock & Watson,
 2007).
 Considering that inflation has both a permanent stochastic trend component
 that has suffered large changes in its variance since the post-war period
 and a serially uncorrelated transitory component with a relatively constant
 variance, Stock & Watson compare different models, including a univariate
 AR model, an Atkeson–Ohanian (2001) random walk, an integrated moving-average
 IMA(1,1), multiple multivariate Backward-looking Phillips curve (PC) and
 a UC-SV model.
 They divide the US inflation data into two samples (1970Q1- 1983Q4 and
 1984Q1-2004Q4) to produce pseudo-out-of-sample forecasts.
 They concluded that in the second period, MSFE has declined substantially,
 meaning that inflation has become relatively easier to forecast.
 However, the traditional AR and PC models have reduced their performance,
 whereas the UC-SV and IMA models appear to hold the upper hand in forecasting.
\end_layout

\begin_layout Standard
Another approach to forecasting was developed in the 80s, embracing Bayesian
 methods for estimating vector autoregressions (Litterman, 1980; Litterman,
 1986; Doan et al., 1984; Sims, 1993) showing that the Bayesian vector autoregres
sion (BVAR) approach had better results than their univariate counterparts.
 Litterman was a pioneer in forecasting macroeconomic variables based on
 Bayesian shrinkage estimation techniques developed in previous decades
 (Shiller, 1970; Stein, 1960).
 Doan, Litterman, and Sims provided unconditional forecasts 1982:12-1983:3
 for the U.S.
 Congressional Budget Office and described how these Bayesian techniques
 with time-varying parameters could be useful for conditional projections
 and to discuss policy alternatives.
 Sims modified Litterman’s model to allow for conditional heteroskedasticity
 and non-normality of disturbances and used it to forecast nine macroeconomic
 variables.
 His model worked pretty well with unconditional predictions, but it felt
 short with conditional predictions, therefore, was not as useful to analyze
 policy interventions.
\end_layout

\begin_layout Standard
The adoption of the Bayesian techniques would lead to an increase in literature
 that would use TVP in modelling and forecasting (Canova, 1993; Cogley and
 Sargent, 2001; Cogley and Sargent, 2002; Primiceri, 2005).
 Canova asserts that due to the short-term changes in financial time series,
 they are better described by stochastic processes that have leptokurtic
 distributions (with heavy tails and a sharper peak) and are conditionally
 heteroskedastic.
 In that sense, the author proposes as a forecasting tool a Bayesian timevarying
 coefficient (TVP-BVAR) model.
 HAssessing forecasting in linear and non-linear Machine Learning models
 for short-term inflation in Perue forecasted the U.S.
 spot exchange rates and spot interest rates from 1986.1 to 1987.51 and concluded
 that the TVP model improves upon RW forecasts at all horizons and for all
 the exchange rates.
 Similarly, Cogley and Sargent used a non-linear TVP-BVAR model to describe
 inflation-unemployment dynamics in the post-war U.S.
 economy.
 They then extended the model to include stochastic volatility to capture
 the drift in the persistence of inflation.
 Primiceri also used a TVP model to capture policy changes in a structural
 vector autoregression (TVP-SVAR) and found evidence of time variation in
 the U.S.
 monetary policy.
\end_layout

\begin_layout Standard
Most of the research and breakthroughs in inflation estimation and forecasting
 have been focused on developed countries.
 However, in the last few decades, literature concerning forecasting in
 the region has also implemented new techniques.
 One example is Gil-Alana & Pestana Barros (2009) who used a Fractional-Cointegr
ated VAR (FCVAR) with structural break to analyse inflation in 18 Latin
 American countries.
 They found out that most countries had a break date around the late 80s
 and early 90s.
 For the FCVAR model, they were inspired by Bos et al.
 (2001), who reviewed the forecasting ability of fractional models.
\end_layout

\begin_layout Standard
Similarly, the most recent work of Pincheira-Brown et al.
 (2019) examines the ability of core inflation to predict consumer price
 index (CPI) annual inflation in eight Latin American countries.
 They used a sample period (January 1995–May 2017) where most countries
 faced changes in their monetary regimes and different international and
 domestic financial crisis.
 Using two nested VAR models, they find out the core inflation is an important
 predictor for headline inflation in the short-run, but does not add much
 information for most countries in the long-run.
\end_layout

\begin_layout Standard
In Peru, there is also literature that has used these innovative approaches
 to forecast inflation.
 Llosa et al.
 (2006) were pioneers in the introduction of Bayesian techniques following
 the methodology of Doan et al.
 (1984) to forecast Peruvian inflation.
 The authors used four specifications for BVARs (which included from five
 (the simplest) to nine variables) using 6 lags for all models.
 The evaluation of the predictive accuracy of each BVAR specification is
 obtained by estimating and recursively predicting.
 The estimation period ranges from August 1995 to November 2001 and the
 forecast period goes from December 2001 to August 2004.
 To evaluate the out-of-sample forecast of the different models, they used
 both the mean square error (MSE) and the U-Theil statistic.
\end_layout

\begin_layout Standard
In general, they find that all models except the BVAR 4 perform quite well
 in predicting inflation in the first out-of-sample period and, on average,
 the BVAR 1 has the best inflation forecasting performance with the lowest
 U-Theil statistic (0.16).
 When it comes to robustness, the BVAR 1 also outperforms all other models.
 Therefore, they conclude that out-of-sample predictions made with BVARs
 favour small BVAR specifications under different possible criteria.
\end_layout

\begin_layout Standard
Vega et al.
 (2009) and Wilkelried (2013) describe the quarterly macroeconometric model,
 known as Modelo de Proyección Trimestral (MPT), used by the BCRP as a reference
 to make policy simulations and projections of key macroeconomic variables
 in Peru.
 The MPT is a dynamic semi-structural model used to explain the aggregate
 behaviour of a small open economy with partial dollarisation.
 It uses linear behavioural equations obtained from non-linear equations
 derived from the optimal behaviour of economic agents subject to preferences,
 technologies, and market structures.
 The MPT has four main blocks (1) Aggregate supply, (2) Aggregate demand,
 (3) Uncovered interest rate parity and (4) Monetary policy rule.
\end_layout

\begin_layout Standard
Apart from being used in forecasting, the main goal of the MPT is simulating
 the response of the main macroeconomic variables (inflation, GDP gap, monetary
 policy interest rate and exchange rate) to different shocks.
 The MPT serves as a great tool for policymakers at the BCRP.
 However, the great number of equations involving it makes it a non-parsimonious
 model, as well as non-replicable for independent analysts and agents outside
 the projection process.
 
\end_layout

\begin_layout Standard
Barrera (2007) uses an Unaggregated Forecasting System (Sistema de Predicción
 Desagregada or SPD) consisting in a Sparse VAR model approach to predict
 Metropolitan Lima CPI.
 A Sparse VAR model can handle a huge number of equations while maintaining
 parsimony in the model by giving emphasis on identifying and modeling only
 the most relevant relationships while ignoring or assigning low weights
 to the rest of the relationships between variables.
 Sparse VAR parameters are usually sensible to outliers.
 However, Barrera proposes a robust multi-equation procedure that presents
 a gain in accuracy in the presence of outliers.
\end_layout

\begin_layout Standard
The author concludes that robust SparseVAR models improve the accuracy of
 all vanilla SparseVAR models for intermediate horizons, especially in three
 of the four types of SparseVAR models.
 Robust projections are less sensitive to outlier data sequences such as
 those experienced during the 1998 ENSO phenomenon and are thus suitably
 designed for the eventual occurrence of this phenomenon in the future.
\end_layout

\begin_layout Standard
Literature concerning inflation forecasting using ML models has mainly focused
 on developed countries.
 For instance, Ülke et al.
 (2018) make inflation predictions using both time series and ML for the
 USA with data between 1984 and 2014.
 The study shows that ML prevails against the time series models in at least
 seven out of the sixteen conditions used.
 On the other hand, Medeiros et al.
 (2022) contributed with a more detailed investigation that included a total
 of 91 countries, with observations from January 1980 to December 2019.
 They compared six models, three traditional models, and three ML models,
 concluding that, overall, ML models, particularly non-linear ones, were
 the best suited for forecasting inflation.
 Although their research was conducted for a large group of countries, the
 results were the same for developing economies like Brazil and Nigeria,
 where the RF model had the best metrics, especially in horizons from three
 to six months.
\end_layout

\begin_layout Standard
One article centred around ML forecasting for a developing country was done
 by Silva & Piazza (2020).
 Their objective was to build accurate forecasts of the Brazilian consumer
 price inflation (IPCA) at multiple horizons spanning from one to twelve
 months (h = 1, ..., 12).
 They used five different supervised ML algorithms: ridge regression, LASSO
 regression, EN, RF, and quantile regression forest (QRF), as well as a
 series of traditional econometric models: RW, VAR, and ARMA.
 Additionally, they employed reduced-form structural models, factor models,
 and survey forecasts.
 The survey forecast was used as benchmark.
 In the shortest horizon (h=1), the best model was the iterated factor model,
 followed by the survey forecast.
 In the third and fourth places were a reduced-form structural model (hybrid
 Phillips curve model) and the VAR (34) model.
 For longer horizons, the survey forecasts still outperformed other models
 in terms of mean squared error (MSE), although very often with equal predictive
 ability to the factor model and the random forest models.
 ML forecasts showed superior predictive power and outperformed the inflation
 forecasts of all traditional econometric approaches.
\end_layout

\begin_layout Standard
Medeiros et al.
 (2016) also published a work on inflation forecasting using ML for Brazil.
 Specifically, the variable to forecast was the National Consumer Price
 Index (IPCA) and the monthly inflation index (IGPM).
 They used 102 monthly predictors, covering production, government debt,
 price indexes, taxes, financial markets, import and export of goods and
 services, government accounts, savings, investment, wages, and international
 variables recovered from the Central Bank of Brazil (BCB), other government
 databases (the FGV, the IBGE, the IPEADATA) and the Bloomberg database.
 The selected period ranged from January 2000 to December 2013, after the
 implementation of the inflation-targeting policy in Brazil.
 The predictions were made using a rolling window.
\end_layout

\begin_layout Standard
The models used in the paper were the autoregressive model (AR), the factor
 model, the LASSO regression, and the adaLASSO specifications for all the
 forecast horizons, which ranged from one to twelve months (h = 1, ..., 12).
 The metrics used to compare the different methods were the root mean squared
 error (RMSE) and the mean absolute error (MAE).
 The results showed that adaLASSO was the best model to forecast the IPCA
 inflation in the horizon from one to four months (h = 1, , 4).
 On the other hand, for longer horizons (h = 5, , 12), the AR and the factor
 models had better predictions.
 Regarding the IGPM inflation, the adaLASSO was again the best model for
 short-period forecasts, but the AR model was best suited for longer horizons.
\end_layout

\begin_layout Standard
Similarly, Rodriguez-Vargas (2020) published another work focused on a developin
g country.
 The author chose the interannual variation rate of the Consumer Price Index
 of Costa Rica as the variable to forecast for one, three, six, and twelve
 months (h = 1, 3, 6, 12).
 By using data from the Central Bank of Costa Rica (BCCR) from January 2003
 to February 2019, he tested a univariate K-nearest neighbors (KNN) model,
 a KNN with explanatory variables, extreme gradient boosting, an RF model,
 and long short-term memory (LSTM) model.
\end_layout

\begin_layout Standard
Using as metrics of comparion the MSE and the Theil index, the results showed
 that at all horizons, the LSTM model produces the most accurate predictions,
 followed by the average of the univariate methods, the forecasts of the
 univariate KNN, and those of extreme gradient boosting and random forests.
 He also concluded that a combination of forecasts can improve the performance
 in comparison with individual forecasts at all horizons, and, most importantly,
 outperforms the forecasts from univariate methods.
\end_layout

\begin_layout Standard
In the next section, we will describe the theoretical framework that is
 going to be used in this document for the in the evaluation and comparison
 of the forecasts.
 Then, we are going to present the methodology as well as the data and the
 different models for modelling inflation, both econometric and machine
 learning models.
\end_layout

\begin_layout Section
Theoretical Framework
\begin_inset CommandInset label
LatexCommand label
name "sec:Third-section"

\end_inset


\end_layout

\begin_layout Standard
In this section, we will describe the criteria of forecast evaluation and
 forecast comparison.
 In the section 3.1, we will describe the two main properties of a good forecast:
 unbiasedness and efficiency.
 In the section 3.2, we will develop the main procedures when comparing forecasts.
 
\end_layout

\begin_layout Standard
Given a 
\begin_inset ERT
status open

\begin_layout Plain Layout

$X_t$
\end_layout

\end_inset

vector of 
\begin_inset ERT
status open

\begin_layout Plain Layout

$k$
\end_layout

\end_inset

 explanatory variables for the variable 
\begin_inset ERT
status open

\begin_layout Plain Layout

$y_t$
\end_layout

\end_inset

 for 
\begin_inset ERT
status open

\begin_layout Plain Layout

$(t = 1, ..., T)$
\end_layout

\end_inset

 we can construct the point forecast of 
\begin_inset ERT
status open

\begin_layout Plain Layout

$y_{t+h}$
\end_layout

\end_inset

 given the information 
\begin_inset ERT
status open

\begin_layout Plain Layout

$I_h$
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
hat{y}_{t+h}=f_{t+h}(X_{t+h}, I_h)$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
we define the forecast error as
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$e_{t+h}=y_{t+h}-
\backslash
hat{y}_{t+h}$
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Forecast evaluation
\end_layout

\begin_layout Standard
Following Granger & Newbold (1986) and Clements & Henry (1998) on forecast
 evaluation, we identify unbiasedness and efficiency as the two main properties
 of a good prediction.
\end_layout

\begin_layout Subsubsection
Unbiasedness
\end_layout

\begin_layout Standard
Unbiasedness can be interpreted as the fact that the optimal forecast under
 a MSFE loss function is the conditional expectation of the variable.
 
\end_layout

\begin_layout Standard
We first introduce the idea of a cost (loss) function given the size of
 the error 
\begin_inset Formula $C(e)$
\end_inset

, then as a natural criterion, given such loss function, we will choose
 the point forecast that minimizes it:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{t+h}=minC(e)
\]

\end_inset


\end_layout

\begin_layout Standard
Then, an unbiased forecast fulfills
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{t+h}=E_{c}(X_{t+h}|I_{h})
\]

\end_inset


\end_layout

\begin_layout Standard
or, equivalently 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
E_{t}(y_{t+h})=\hat{y}_{t+h|t}=f_{t+h}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $E_{t}(.)$
\end_inset

 is the conditional expectation given information at time 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
The result of the expected forecast error should be equal to zero, which
 means that, on average, the forecast should be correct.
\end_layout

\begin_layout Standard
To test for unbiasedness, we should consider the regression 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}=\alpha+\beta\hat{y}_{t+h}+\epsilon_{t+h}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
for 
\begin_inset Formula $t=T,...,T+H-h$
\end_inset

 where 
\begin_inset Formula $h$
\end_inset

 is the forecast horizon and 
\begin_inset Formula $T,...,T+H-h$
\end_inset

 is the evaluation sample.
\end_layout

\begin_layout Standard
For the forecasts
\begin_inset Formula $\hat{y}_{t+h}$
\end_inset

$ to be unbiased we can use a robust F-test to test the condition 
\begin_inset Formula $\alpha=0,\beta=1$
\end_inset

.
 We can also write the following equation: 
\begin_inset Formula 
\begin{equation}
e_{t+h}=y_{t+h}-\hat{y}_{t+h}=\epsilon_{t+h}+\tau
\end{equation}

\end_inset

 and test for 
\begin_inset Formula $\tau=0$
\end_inset

 with a robust t-test for 
\begin_inset Formula $\hat{y}_{t+h}$
\end_inset

 to be unbiased.
\end_layout

\begin_layout Subsubsection
Efficiency
\end_layout

\begin_layout Standard
This criterion refers to the efficient use of all the available information.
 If the forecast is inefficient, then it can be improved upon with a better
 specification in the forecasting model.
 We can test weak or strong efficiency.
 Weak efficiency means that the optimal 
\begin_inset Formula $h$
\end_inset

-steps forecast error 
\begin_inset Formula $(e_{t+h})$
\end_inset

 should be correlated, across time, at most of order 
\begin_inset Formula $h-1$
\end_inset

 and should be uncorrelated with available information at the time the forecast
 was made.
 This property can be evaluated by fitting a moving-average of order $h-1$
 to the 
\begin_inset Formula $h$
\end_inset

-steps ahead forecast error 
\begin_inset Formula $(e_{t+h})$
\end_inset

 and then testing if the residuals are white noise.
\end_layout

\begin_layout Standard
Strong efficiency can be easily assessed by testing 
\begin_inset Formula $\gamma=0$
\end_inset

 in 
\begin_inset Formula 
\begin{equation}
e_{t+h}=\gamma'z_{t}+\epsilon_{t+h}
\end{equation}

\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 are potentially relevant variables for the explanation of forecast errors.
 If the 
\begin_inset Formula $h-$
\end_inset

steps ahead forecast errors 
\begin_inset Formula $(e_{t+h})$
\end_inset

 are strongly efficient, then no indicators 
\begin_inset Formula $z_{t}$
\end_inset

 at the time the forecast was formulated can improve them and, therefore,
 explain the 
\begin_inset Formula $h-$
\end_inset

step forecast error.
\end_layout

\begin_layout Subsection
Forecast comparison
\end_layout

\begin_layout Standard
The most common approach when ranking forecasts is to compare their accuracy.
 That is, the measure of how accurately a given forecast matches actual
 values.
 In this subsection we will first portray deterministic comparison methods
 such as the root mean squared forecast error (RMSFE) and the mean absolute
 percentage error (MAPE).
 Then, following Ghysels & Marcellino (2018) we will outline a forecast
 comparison test that evaluates whether the difference between forecasts
 is statistically significant, namely the Diebold-Mariano test.
\end_layout

\begin_layout Subsubsection
Root Mean Square Forecast Error (RMSFE)
\end_layout

\begin_layout Standard
Given the forecast error 
\begin_inset Formula 
\[
e_{t+h}=y_{t+h}-\hat{y}_{t+h}
\]

\end_inset

 The MSFE can be depicted as 
\begin_inset Formula 
\[
MSFE=\frac{1}{T}\sum_{t=1}^{T}(y_{t+h}-\hat{y}_{t+h})^{2}
\]

\end_inset

 
\begin_inset Formula 
\[
MSFE=\frac{1}{T}\sum_{t=1}^{N}e_{t+h}^{2}
\]

\end_inset

 Therefore, the RMSFE is defined by 
\begin_inset Formula 
\begin{equation}
RMSFE=\sqrt{MSFE}=\sqrt{\frac{1}{T}\sum_{t=1}^{N}e_{t+h}^{2}}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection
Mean absolute percentage error (MAPE)
\end_layout

\begin_layout Standard
The mean absolute percentage error is a loss function defined as 
\begin_inset Formula 
\[
MAPE=\frac{1}{T}\sum_{t=1}^{T}|\frac{y_{t+h}-\hat{y}_{t+h}}{y_{t+h}}|
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
MAPE=\frac{1}{T}\sum_{t=1}^{T}|\frac{e_{t+h}}{y_{t+h}}|
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Diebold-Mariano test
\end_layout

\begin_layout Standard
Proposed by Diebold & Mariano (1995), this test relaxes the requirement
 of forecasts errors and can compare directly general loss functions.
 Given the forecast errors 
\begin_inset Formula $e_{i}$
\end_inset

 of the competing forecast models 
\begin_inset Formula $i$
\end_inset

 for 
\begin_inset Formula $i=1,2$
\end_inset

, we can define the loss-differential as 
\begin_inset Formula 
\[
d_{j}=C(e_{1}j)-C(e_{2}j)
\]

\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is the loss function, which, for example, can be the quadratic loss 
\begin_inset Formula $C(e)=e^{2}$
\end_inset

 or the absolute loss
\begin_inset Formula $C(e)=|e|$
\end_inset

.
 We want to test the null hypothesis 
\begin_inset Formula 
\[
H_{0}:E(d_{j})=0
\]

\end_inset

 against the alternative 
\begin_inset Formula 
\[
H_{1}:E(d_{j})\neq0
\]

\end_inset

 Then, the Diebold-Mariano statistic is defined as 
\begin_inset Formula 
\begin{equation}
DM=H^{1/2}\frac{\sum_{j=1}^{H}d_{j}/H}{\sigma_{d}}=H^{1/2}\frac{\bar{d}}{\sigma_{d}}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\sigma_{d}$
\end_inset

 is the variance of $
\backslash
hat{d}$ , which can be estimated given that
\begin_inset Formula 
\[
\hat{\sigma_{d}}=(\gamma_{0}+2\sum_{i=1}^{h-1}\gamma_{i})
\]

\end_inset

 
\begin_inset Formula 
\[
\gamma_{k}=H^{-1}\sum_{t=k-1}^{H}(d_{t}-\bar{d})d_{t-k}-\bar{d})
\]

\end_inset

 where 
\begin_inset Formula $h$
\end_inset

 is the forecast horizon, meaning that in 
\begin_inset Formula $h=1$
\end_inset

 there is no correlation, so the standard formula for the estimation of
 the variance
\begin_inset Formula $\gamma_{0}$
\end_inset

 can be used.
\end_layout

\begin_layout Section
Methodology and data
\begin_inset CommandInset label
LatexCommand label
name "sec:Forth-section"

\end_inset


\end_layout

\begin_layout Standard
In this section we will first present the data and discuss the models that
 are going to be compared as well as the methodology to assess the models’
 performance.
 There are two main periods to be considered for the forecasts.
 The first one ranges from 2007Q2-2008Q4, which was a period where energy
 and commodity prices, especially oil prices, boosted inflationary pressure,
 which greatly impacted both advanced and emerging economies (International
 Monetary Fund, 2008).
 The second period ranges from 2014Q2 to 2015Q4, were the was a small increase
 in local inflation due to foodstuffs and electricity rates, as well as
 major volatility (BCRP, 2015).
 Forecasts will we done in the short horizon, ranging from from a 3-months
 period, 6-month period and a year 
\begin_inset Formula $(h=3,6,12)$
\end_inset

.
\end_layout

\begin_layout Standard
Models will we rank based on their RMSFE and their MAPE for each period
 in all three horizons.
 We will then use the Diebold-Mariano test to compare each model against
 the benchmark RW and see how each model competes the benchmark RW.
\end_layout

\begin_layout Standard
We are considering the three econometric models (RW, VAR, ARIMA) and four
 machine learning models (LASSO, Ridge, EN and RF).
\end_layout

\begin_layout Subsection
Data
\end_layout

\begin_layout Standard
Our analysis employs twelve distinct series, notably featuring the Metropolitan
 Lima Price Index represented in monthly percentage change over a twelve-month
 period (CPI).
 Additionally, we incorporate the following indices: Food and Energy CPI,
 CPI Excluding Food and Energy, Core CPI, Non-core CPI, Food and Beverages
 CPI, Imported CPI, and the Multilateral real exchange rate.
 Furthermore, our model integrates external variables such as Maize (US$
 per tonne), Wheat (US$ per tonne), Soybean oil (US$ per tonne), and Crude
 Oil (US$ per barrel).
 All data utilized in this study is sourced from the database of the Central
 Reserve Bank of Peru (BCRP), with a consistent monthly frequency.
\end_layout

\begin_layout Standard
The graphical representations in Figure 1 and 2 depict the plotted trajectories
 of these variables over time.
 Notably, the graphs reveal three distinct periods characterized by heightened
 volatility: the periods of 2007-2008, 2014-2015, and most recently, 2022-2023.
 For the purpose of our forecasting analysis, we specifically include the
 initial and most recent periods of heightened volatility within our designated
 forecasting windows.
\end_layout

\begin_layout Subsubsection
Predictors for headline inflation
\end_layout

\begin_layout Standard
In examining predictors for Headline Inflation (CPI), our analysis encompasses
 a comprehensive set of variables, including but not limited to: CPI tradables,
 CPI non-tradables, CPI Core, CPI Non-Core, CPI Food and Energy, CPI excluding
 Food and Energy, CPI Food and Beverages, CPI excluding Food and Beverages,
 CPI Core excluding Food and Beverages, CPI Imported, Wholesale Price Index,
 Reserve Requirement Rate, Monetary Policy Rate, Circulating Currency, Net
 International Reserves, and Minimum Wage.
\end_layout

\begin_layout Standard
Figure 3 and Figure 4 present the correlation analysis among these variables,
 depicted in heatmap format.
 As delineated in Figure 3, headline inflation exhibits pronounced correlations
 with CPI Non-Tradables, CPI Food and Energy, CPI Food and Beverages, and
 to a lesser extent, with other CPI components.
 Notably, the correlation with the Monetary Policy Rate appears relatively
 modest.
 Upon incorporating lagged variables, depicted in Figure 4, headline inflation
 manifests a robust correlation with its own lagged values.
 Conversely, the correlation between CPI and lagged values of other variables
 appears generally weak and statistically non-significant.
\end_layout

\begin_layout Subsubsection
Predictors for Core Inflation
\end_layout

\begin_layout Standard
For the prediction of Core Inflation, we consider an array of potential
 predictors encompassing various dimensions of consumer price dynamics.
 These predictors include, but are not limited to: CPI tradables, CPI non-tradab
les, overall CPI, CPI Non-Core, CPI Food and Energy, CPI excluding Food
 and Energy, CPI Food and Beverages, CPI excluding Food and Beverages, CPI
 Core excluding Food and Beverages, CPI Imported, Wholesale Price Index,
 Reserve Requirement Rate, Monetary Policy Rate, Circulating Currency, Net
 International Reserves, and Minimum Wage.
\end_layout

\begin_layout Standard
In Figure 3, we present the correlation analysis between CPI Core and its
 predictors.
 Notably, the correlation appears strongest with CPI Excluding Food and
 Energy, CPI Core Excluding Food and Beverages, and CPI Non-Tradables.
 Furthermore, we observe comparatively lower correlations with other CPI
 variables.
 Upon incorporating lagged variables, as illustrated in Figure 5, the correlatio
n analysis reveals additional insights.
 Specifically, CPI Core demonstrates correlations with lagged values of
 CPI Non-Tradables and overall CPI, and notably, in contrast to Headline
 Inflation, with the lagged values of the Monetary Policy Rate.
\end_layout

\begin_layout Subsection
Econometric models
\end_layout

\begin_layout Standard
In this subsection we will describe two frequentist approaches to inflation
 forecasting: an univariate random walk and a multivariate vector autoregression
, as well as Bayesian vector autoregression.
\end_layout

\begin_layout Subsubsection
Random Walk (RW)
\end_layout

\begin_layout Standard
Out of all the models, the RW is simplest.
 Consider the non-stationary model 
\begin_inset Formula 
\begin{equation}
y_{t}=y_{t-1}+\epsilon_{t}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
For any $h$ periods ahead being forecasted it assumes that the inflation
 rate is predicted with the last observation of itself.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{y}_{T+h}=y_{T}
\end{equation}

\end_inset

 The forecast errors are 
\begin_inset Formula 
\[
e_{T+h}=\epsilon_{t+1}+\epsilon_{t+2}+...+\epsilon_{t+h}
\]

\end_inset

 Hence, the variance of the forecast errors are 
\begin_inset Formula 
\[
Var(e_{T+h})=h\sigma_{\epsilon}^{2}
\]

\end_inset


\end_layout

\begin_layout Subsubsection
Vector Autoregression (VAR)
\end_layout

\begin_layout Standard
Given a VAR(p) model of 
\begin_inset Formula $k$
\end_inset

 variables
\begin_inset Formula $y_{1t},...,y_{kt}$
\end_inset

 grouped in a 
\begin_inset Formula $(k\times1)$
\end_inset

 vector 
\begin_inset Formula $y_{t}=(y_{1}t,...,y_{k}t)'$
\end_inset

, it can be represented as: 
\begin_inset Formula 
\begin{equation}
y_{t}=\mu+\Phi_{1}y_{t-1}+...+\Phi_{p}y_{t-p}+\epsilon_{t}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\epsilon_{t}\sim WN(0,\Sigma)
\end{equation}

\end_inset

 where 
\begin_inset Formula $\Sigma$
\end_inset

 is the 
\begin_inset Formula $(k\times k)$
\end_inset

variance-covariance matrix and 
\begin_inset Formula 
\[
\epsilon_{t}=(\epsilon_{1t},...,\epsilon_{kt})'
\]

\end_inset

 is a vector of error terms.
 Each error is uncorrelated over time and homoskcedastic, but can be correlated
 with errors in other equations, meaning
\begin_inset Formula $\epsilon_{t}$
\end_inset

 is a multivariate white noise process.
\end_layout

\begin_layout Standard
Note that the total number of parameters grows very fast as the number of
 variables k increases.
 There are 
\begin_inset Formula $k$
\end_inset

 parameters in the intercept, 
\begin_inset Formula $k^{2}p$
\end_inset

 coefficients of the lagged variables and 
\begin_inset Formula $k(k+1)/2$
\end_inset

 parameters in the variance-covariance matrix.
\end_layout

\begin_layout Standard
Using the Wold decomposition theorem, Equation (12) can be represented as
 VMA(
\begin_inset Formula $\infty$
\end_inset

)
\begin_inset Formula 
\begin{equation}
y_{t}=\Psi(L)\epsilon_{t}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\Psi(L)=(I+\Psi_{1}L+\Psi_{2}L^{2}+...)$
\end_inset

 is a matrix polynomial in L.
 
\end_layout

\begin_layout Standard
The optimal point forecast for the VAR(p) is the extension of the univariate
 case, meaning that 
\begin_inset Formula 
\[
\hat{y}_{T+h}=\Phi_{1}\hat{y}_{T+h-1}+...+\Phi_{p}\hat{y}_{T+h-p}
\]

\end_inset

 Using the VMA(
\begin_inset Formula $\infty$
\end_inset

) representation in (17).
 The optimal forecast can be rewritten as 
\begin_inset Formula 
\begin{equation}
\hat{y}_{T+h}=\sum_{j=0}^{\infty}\Psi_{j+h}\epsilon_{T-j}
\end{equation}

\end_inset

 with the following forecast error 
\begin_inset Formula 
\begin{equation}
e_{t+h}=\sum_{j=0}^{h-1}\Psi_{j}\epsilon_{t+H-j}
\end{equation}

\end_inset

 and the variance-covariance of the forecast error 
\begin_inset Formula 
\begin{equation}
V(e_{t+h})=\Sigma+\Psi_{1}\Sigma\Psi_{1}'+...+\Psi_{h-1}\Sigma\Psi_{h-1}'
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Machine Learning models
\end_layout

\begin_layout Standard
In this subsection we will describe the ML models.
 We will describe two linear methods, namely LASSO regression and Elastic
 Net, and one non-linear method: Random Forest.
\end_layout

\begin_layout Subsubsection
LASSO Regression
\end_layout

\begin_layout Standard
The least absolute shrinkage and selection operator (LASSO) was first developed
 as a frequentist shrinkage method by Tibshirani (1996).
 In ML it is used as method for feature selection and regularization.
 The LASSO regression adds a penalty term which depends on the absolute
 value of the regression coefficients.
\end_layout

\begin_layout Standard
Given the following linear regression model 
\begin_inset Formula 
\[
y_{t}=\beta x_{t}+\epsilon_{t}
\]

\end_inset

 where 
\begin_inset Formula $y$
\end_inset

 is an 
\begin_inset Formula $N\times1$
\end_inset

 vector of dependant variables, X is an 
\begin_inset Formula $N\times K$
\end_inset

 matrix of explanatory variables, 
\begin_inset Formula $\beta=(\beta_{1},...,\beta_{k})$
\end_inset

 is a vector regression coefficients and 
\begin_inset Formula $\epsilon$
\end_inset

 is a vector of errors.
 It is possible that 
\begin_inset Formula $K$
\end_inset

 is relatively large compared to 
\begin_inset Formula $N$
\end_inset

.
 In those cases, the LASSO estimates are chosen to minimize
\begin_inset Formula 
\begin{equation}
LASSO=\min_{\beta}(\sum_{i=1}^{N}(y_{i}-\sum_{j=1}^{K}\beta_{j}x_{i}j)^{2}+\lambda\sum_{i=1}^{K}|\beta_{i}|)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
LASSO=\min_{\beta}(RSS+\lambda\sum_{i=1}^{K}|\beta_{i}|)
\end{equation}

\end_inset

 where RSS is the residual sum of squares and the second term 
\begin_inset Formula $\lambda\sum_{i=1}^{K}|\beta_{i}|$
\end_inset

 is called the shrinkage penalty and is a regularization type 
\begin_inset Formula $\ell_{1}$
\end_inset

.
 It is null when
\begin_inset Formula $\beta_{1},...,\beta_{k}$
\end_inset

 are set to zero.
 
\begin_inset Formula $\lambda\geq0$
\end_inset

 is a tuning parameter, it is use to control the impact of the coefficients
 in the regression.
 When 
\begin_inset Formula $\lambda=0$
\end_inset

 there is no penalty effect.
 The greater 
\begin_inset Formula $\lambda$
\end_inset

 is, the bigger the penalty, that means that more coefficients 
\begin_inset Formula $\beta_{i}$
\end_inset

 will be equaled to zero.
 Following the literature, 
\begin_inset Formula $\lambda=0.5$
\end_inset

 for this document.
 LASSO yields sparse models, which are models that involve only a subset
 of the variables.
\end_layout

\begin_layout Standard
The process of reducing variables is called feature selection.
 Once it is completed, we realize a k-fold cross-validation that splits
 the data into test and training samples.
 Each iteration, the k-fold is used as the testing set, and the remaining
 k-1 folds are used as the training set.
 This means that the ML model is trained k times, each time using a different
 fold.
 Upon completion, we proceed to do the point forecasts.
 
\end_layout

\begin_layout Subsubsection
Ridge Regression
\end_layout

\begin_layout Standard
Consider the same linear regression model as in LASSO, the ridge coefficients
 are chosen by imposing a penalty on the squared estimates:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Ridge=\min_{\beta}(RSS+\lambda\sum_{i=1}^{K}\beta_{i}^{2})
\end{equation}

\end_inset

 where the term 
\begin_inset Formula $\lambda\sum_{i=1}^{K}\beta_{i}^{2}$
\end_inset

 is a regularization of type
\begin_inset Formula $\ell_{2}$
\end_inset

 and 
\begin_inset Formula $\lambda$
\end_inset

 is the tuning parameter.
\end_layout

\begin_layout Standard
As 
\begin_inset Formula $\lambda$
\end_inset

 increases, the coefficients 
\begin_inset Formula $\beta_{1},...,\beta_{k}$
\end_inset

 will approach zero.
 
\begin_inset Formula $\lambda$
\end_inset

 will be set to 0.5 in this document.
 Notice that, unlike the LASSO regression, the estimates cannot be zero.
 That means that given a model with a large number of parameters, the ridge
 regression will always generate a model involving all predictors, but will
 reduce their magnitudes by making the coefficients are very small.
 After the estimation with the 
\begin_inset Formula $\ell_{2}$
\end_inset

 penalty, the k-fold cross validation and the train-test split begins.
 The ML is trained k times and then we start with the forecast procedure.
 
\end_layout

\begin_layout Subsubsection
Elastic Net
\end_layout

\begin_layout Standard
Elastic Net is another regularization technique used in linear regressions.
 It combines both 
\begin_inset Formula $\ell_{1}$
\end_inset

 and 
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization's.
 That means that the estimates will be chosen by 
\begin_inset Formula 
\begin{equation}
Elastic\ Net=\min_{\beta}(RSS+\lambda(\alpha\sum_{i=1}^{K}|\beta_{i}|+(1-\alpha)\sum_{i=1}^{K}\beta_{i}^{2}))
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
Elastic\ Net=\min_{\beta}(RSS+\lambda(\alpha\ell_{1}+(1-\alpha)\ell_{2})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\ell_{1}$
\end_inset

 and 
\begin_inset Formula $\ell_{2}$
\end_inset

 correspond to their specific regularization and 
\begin_inset Formula $\alpha$
\end_inset

 is a tuning parameter that will measure the weight of the
\begin_inset Formula $\ell_{1}$
\end_inset

 penalty.
 If 
\begin_inset Formula $\alpha=0$
\end_inset

, then we will have a Ridge regression.
 If it is 1, then the EN transforms to a LASSO regression.
 By convention, we will use 
\begin_inset Formula $\alpha=0.5$
\end_inset

 which will give an equal proportion to each regularization technique.
 The other tuning parameter 
\begin_inset Formula $\lambda$
\end_inset

 presents the weight of the combined penalties.
 It is also set to 0.5.
 
\end_layout

\begin_layout Standard
After the selection of the variables, the k-fold cross-validation starts
 the train-test split is executed.
 Once the ML model has been trained k times, we can start the forecasting
 procedure.
\end_layout

\begin_layout Subsubsection
Random Forest
\end_layout

\begin_layout Standard
RF is a non-linear technique that can be used to create a large number of
 regression trees.
 This regressions trees divide the observations into regions where the predictor
 values are similar.
 The choice of the splitting points is done by minimizing a loss function.
 In this way the overall variance is reduce by averaging many of these regressio
n trees.
 To understand the concept better, let us first compare the typical linear
 regression model 
\begin_inset Formula 
\begin{equation}
y=\sum_{i=1}^{p}X_{i}\beta_{i}
\end{equation}

\end_inset

 where there is linearity in the coefficients.
 That means that the relationship between the dependant and the independent
 variables is a linear combination of $x$ and $
\backslash
beta$ .
 Now let us assume a regression tree model in the form 
\begin_inset Formula 
\begin{equation}
y=\sum_{m=1}^{M}c_{m}\cdot1_{(x\in R_{m})}
\end{equation}

\end_inset

 where 
\begin_inset Formula $(R_{1},...,R_{M})$
\end_inset

 correspond to the partition regions for the observations.
 To construct the regression tree, a set of possible values 
\begin_inset Formula $(x_{1},...,x_{p})$
\end_inset

 is split into 
\begin_inset Formula $M$
\end_inset

 possible non-overlapping regions 
\begin_inset Formula $(R_{1},...,R_{M})$
\end_inset

.
 Then, for every observation that falls into the region 
\begin_inset Formula $R_{m}$
\end_inset

, we will make the same prediction, which is the average of the response
 values for the training observations in 
\begin_inset Formula $R_{m}$
\end_inset

.
\end_layout

\begin_layout Standard
In the context of RF, each time a split in a tree is done, a random sample
 of $m$ predictors is chosen as split possible candidates from the full
 set of 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $M$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 predictors.
 This split is allowed to use only one of those 
\begin_inset Formula $m$
\end_inset

 predictors.
 Usually, the number of predictors assessed at each split is approximately
 the square root of the total number of predictors 
\begin_inset Formula $M$
\end_inset

, meaning 
\begin_inset Formula $m=\sqrt{M}$
\end_inset

, which differentiates RF from bootstrapping, where the split considers
 the full sample 
\begin_inset Formula $m=M$
\end_inset

 each time.
 The RF way of splitting predictors will typically be useful when we have
 a large number of correlated predictors in our dataset, which could be
 the case of inflation.
\end_layout

\begin_layout Standard
If the relationship between the predictor 
\begin_inset Formula $x$
\end_inset

 and the response variable 
\begin_inset Formula $y$
\end_inset

 is linear, then (30) will be the ideal model to use, as it will outperform
 the regression tree.
 However, in the presence of non-linearity's in the features and the response,
 then the model (31) will have better results.
 If there is a presence of non-linear relationships between inflation and
 its predictors during the analyzed period, then the RF model will outperform
 all linear econometric and ML models.
\end_layout

\begin_layout Subsection
Model implementation
\end_layout

\begin_layout Standard
All models are implemented using the Scikit-Learn and XGBoost package in
 Python.
 Linear models are imported as the Lasso, Ridge, and ElasticNet functions
 respectively.
 Non-linear models are imported from RandomForestRegressor and XGBRegressor.
 All models are implemented with a random state = 2023.
 A cross validation followed by a grid-search is implemented using the TimeSerie
sSplit and GridSearchCV modules from Scikit-Learn.
 The models best parameters using the grid-search are as follows:
\end_layout

\begin_layout Itemize
Ridge: 
\begin_inset Formula $\lambda=0.1$
\end_inset

 
\end_layout

\begin_layout Itemize
Lasso: 
\begin_inset Formula $\lambda=0.1$
\end_inset


\end_layout

\begin_layout Itemize
Elastic Net: 
\begin_inset Formula $\lambda=0.1,\alpha=0.1$
\end_inset


\end_layout

\begin_layout Itemize
Random Forest: 
\begin_inset Formula $maxdepth=10,nestimators=150$
\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
In this section we will present the main results after running the models.
 We are comparing both the RMSE and the MAPE through horizons 1 to 12 for
 two periods January 2009 to December 2009 and January 2019 to December
 2019.
 Both metrics are presented using the RW as the benchmark.
 In order to test for robustness, we are making our predictions for two
 variables: Headline Inflation (Lima Metropolitan Precious Index (monthly
 % change) - CPI) and Core Inflation (Metropolitan Lima price index (monthly
 % change) - Underlying CPI).
 We are using the first and second lags of the other variables as predictors.
 We will first present the results for the Headline Inflation as the target
 variables in both 2009 and 2019.
 Then, we will present results for the Core Inflation in the same periods.
\end_layout

\begin_layout Subsection
Headline inflation
\end_layout

\begin_layout Subsubsection
Forecasting from January 2009 to December 2009
\end_layout

\begin_layout Standard
We can see in figure 3 the results for the RMSE.
 It is noticeable that for all models it lies under 1, implying all models
 presented (linear and non-linear) are improving upon the RW prediction.
 There are three groups clearly identified.
 First, we observe that the RF model has the best performance of all models
 for all horizons (Except for a tie on 
\begin_inset Formula $h=2$
\end_inset

 with the EN model).
 There is even a slight decrease in the RMSE after the second horizon, unlike
 other models.
 Second, we see all linear ML models competing for the second place.
 Finally, the VAR model has the worst performance overall, with an increasing
 RMSE from around 0.3 in 
\begin_inset Formula $h=1$
\end_inset

 to 0.5 in 
\begin_inset Formula $h=12$
\end_inset

.
 
\end_layout

\begin_layout Standard
In figure 4 we have plotted the MAPE for all models as a percentage of the
 RW RMSE.
 We see that the RF has overall an absolute error of less than 10%.
 It is followed closely in the nearest horizons by the EN and Lasso linear
 ML models, which have almost the same results up to 
\begin_inset Formula $h=8$
\end_inset

.
 The Lasso model performs better than the other linear models only in the
 most distant horizons (
\begin_inset Formula $h=8$
\end_inset

 and higher).
 It also should be noted that the MAPE is higher in both the shorter and
 longer horizons, following a similar pattern for all linear models.
 
\end_layout

\begin_layout Standard
Regarding the main predictors, we will discuss the ones elected by the best
 two models overall: the RF model and the EN model.
 The linear ML model is choosing typical macroeconomic variables related
 to inflation such as CPI Tradables,the Monetary Policy Reference Rate and
 the price of commodities such as Soybean Oil and Wheat.
 The RF model, on the other hand, chooses as its main predictor the real
 index of the minimum living wage (MLW), and then, with a lesser strength,
 the CPI Non-tradables and the CPI Food and Beverages.
 In Annex 1 we can see all 10 predictors for both models.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"
literal "false"

\end_inset

Alami, Y., El Idrissi, I., Bousselhami, A., Raouf, R.
 and Boujettou, H.
 (2021).
 “Macroeconomic impacts of f
\backslash
input{./input/external-table.tex}iscal shocks on the Moroccan economy: a disaggreg
ated SVAR analysis,”
\begin_inset space \space{}
\end_inset


\shape italic
Journal of Bussiness amd Socio-Economic
\shape default
 
\series bold
2(2)
\series default
, 137-152.
 doi: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://doi.org/10.1108/JBSED-08-2021-0112
\end_layout

\end_inset

.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "anderson77"
literal "false"

\end_inset

Anderson, O.
 D.
 (1977).
 Time Series Analysis and forecasting: Another look at the box-jenkins approach.
 Journal of the Royal Statistical Society.
 Series D (The Statistician), 26(4), 285–303.
 doi: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://doi.org/10.2307/2987813 
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "bcrp"
literal "false"

\end_inset

Banco Central de la República del Perú.
 (n.d.).
 Sobre el BCRP.
 Retrieved September 9, 2023, from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://www.bcrp.gob.pe/docs/sobre-el-bcrp/folleto/folleto-institucion al-00.pdf.
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "bcrp2015"
literal "false"

\end_inset

Banco Central de la República del Perú.
 (2015).
 (rep.).
 
\backslash
textit{Inflation Report.
 Recent trends and macroeconomic forecasts 2015-2017.} Retrieved October
 3, 2023, from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://www.bcrp.gob.pe/eng-docs/Monetary-Policy/Inflation-Report/2015/inflation-re
port-may-2015.pdf.
\end_layout

\end_inset

 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "bos2001"
literal "false"

\end_inset

Bos, C.
 S., Franses, P.
 H., & Ooms, M.
 (2001).
 Inflation, forecast intervals and long memory regression models.
 
\backslash
textit{International Journal of Forecasting, 18(2)}, 243–264.
 doi: 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://doi.org/10.1016/s0169-2070(01)00156-x
\end_layout

\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{page}{1}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
renewcommand{
\backslash
thepage}{F-
\backslash
arabic{page}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{landscape}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
Figure 1.
 Time Series in Annual Growth Rates (1993Q1-2019Q4).
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{landscape}
\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{landscape}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{landscape}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\end_body
\end_document
