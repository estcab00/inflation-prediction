{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9015fd4b",
   "metadata": {},
   "source": [
    "# 3. Regression\n",
    "In this notebook we will finally run our regression models. For that purpose, we are importing the necessary libraries and functions from our ```modules``` folder. We are also importing our extracted dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380ac8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basic Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from functools import reduce\n",
    "\n",
    "# Statsmodels\n",
    "import statsmodels.api as sm\n",
    "import pmdarima as pmd\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.vector_ar.var_model import VARResults\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    "    precision_score\n",
    "\n",
    ")\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e8288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import our own functions\n",
    "import sys\n",
    "sys.path.append('../../..')  # Move two levels up to the project root\n",
    "from modules.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867ff30c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/df_raw_h19.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/df_raw_h19.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFecha\u001b[39m\u001b[38;5;124m'\u001b[39m], index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFecha\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mtail()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/df_raw_h19.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../input/df_raw_h19.csv', parse_dates=['Fecha'], index_col='Fecha')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lags = pd.read_csv('../input/df_lags_h19.csv', parse_dates=['Fecha'], index_col='Fecha')\n",
    "df_lags.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b191b",
   "metadata": {},
   "source": [
    "## 3.1 Benchmark models\n",
    "\n",
    "In the first section, we first run our benchmark econometric models: ```Random Walk (RW)```,  ```Autoregressive Integrated Moving Average (ARIMA)``` and ```Vector Autoregression (VAR)``` processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a5c2c",
   "metadata": {},
   "source": [
    "### 3.1.1 Random Walk (RW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51596e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_horizons = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "# We define our target variable\n",
    "target = 'CPI'\n",
    "\n",
    "# We only use CPI as Random Walk is an univariate process\n",
    "df_CPI = pd.DataFrame(df_lags.CPI)\n",
    "\n",
    "# We create our train and test set\n",
    "train_set = df_CPI[df_CPI.index < '2019-01-01']\n",
    "test_set  = df_CPI[df_CPI.index >= '2019-01-01']\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for h in forecast_horizons:\n",
    "    # We get the values h horizons before\n",
    "    predicted_value = train_set.iloc[-h, 0]\n",
    "\n",
    "    # We save it for horizon h\n",
    "    predictions[h] = predicted_value\n",
    "\n",
    "predicted = pd.DataFrame([predictions]).transpose().reset_index()\n",
    "\n",
    "predicted.columns = ['Horizon', 'Prediction']\n",
    "\n",
    "predicted = predicted.set_index(test_set.index)\n",
    "\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb796d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our results dataframe, concatenating the predicted and the actual values\n",
    "results = pd.concat([predicted, test_set[target]], axis=1)\n",
    "results.rename(columns={'Horizon': 'Horizon', 'Prediction': 'Predicted', 'CPI': 'Actual'}, inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a06604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get our metrics using our function\n",
    "RMSE_rw, MAPE_rw = get_metrics(results, 'RW')\n",
    "metrics_rw = pd.concat([RMSE_rw, MAPE_rw], axis = 1)\n",
    "metrics_rw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1207a96",
   "metadata": {},
   "source": [
    "### 3.1.2 Autoregressive Integrated Moving Average (ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only use CPI as Random Walk is an univariate process\n",
    "df_CPI = pd.DataFrame(df_lags.CPI)\n",
    "\n",
    "# We create our train and test set\n",
    "train_set = df_CPI[df_CPI.index < '2019-01-01']\n",
    "test_set  = df_CPI[df_CPI.index >= '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f2a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find the best SARIMA model\n",
    "autoarima = pmd.auto_arima(\n",
    "        y = train_set,\n",
    "        start_p=1,\n",
    "        start_q=0,\n",
    "        seasonal=True,\n",
    "        max_p=12,\n",
    "        max_d=1,\n",
    "        max_q=6,\n",
    "        max_P=12,\n",
    "        max_D=1,\n",
    "        max_Q=6,\n",
    "        m=4,\n",
    "        n_jobs=-1,\n",
    "        suppress_warnings=True,\n",
    "        )\n",
    "\n",
    "# We indicate the seasonal order for monthly data\n",
    "seasonal_order = (1, 1, 1, 12)\n",
    "\n",
    "# We create our ARIMA model\n",
    "model = SARIMAX(train_set,\n",
    "                order=autoarima.order,\n",
    "                seasonal_order=autoarima.seasonal_order,\n",
    "                enforce_stationarity = False,\n",
    "                enforce_invertibility = False)\n",
    "        \n",
    "# We fit the model\n",
    "model_fit = model.fit()\n",
    "\n",
    "# We forecast for the next 12 horizons\n",
    "forecast_values = model_fit.get_forecast(steps=12)\n",
    "predicted = pd.DataFrame(forecast_values.predicted_mean, index = test_set.index)\n",
    "\n",
    "# We create our results dataframe, concatenating the predicted and the actual values\n",
    "results = pd.concat([predicted, test_set[target]], axis=1)\n",
    "results.rename(columns={'predicted_mean': 'Predicted', 'CPI': 'Actual'}, inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get our metrics using our function\n",
    "RMSE_arima, MAPE_arima = get_metrics(results, 'ARIMA')\n",
    "metrics_arima= pd.concat([RMSE_arima, MAPE_arima], axis = 1)\n",
    "metrics_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318374f2",
   "metadata": {},
   "source": [
    "### 3.1.3 Vector autoregression (VAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f74cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our target variable, as well as our train and test set\n",
    "target = 'CPI'\n",
    "train_set = df[df.index < '2019-01-01']\n",
    "test_set  = df[df.index >= '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We model our VAR including up to two lags\n",
    "model_var = VAR(df)\n",
    "model_fit = model_var.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037addd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We forecast for the next 12 months\n",
    "preds = model_fit.forecast(df.values[-2:], 12)\n",
    "preds = pd.DataFrame(preds, index = test_set[target].index)[0]\n",
    "\n",
    "# We create our results dataframe, concatenating the predicted and the actual values\n",
    "results = pd.concat([preds, test_set[target]],axis=1)\n",
    "results.rename(columns={'CPI': 'Actual', 0: 'Predicted'}, inplace=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get our metrics using our function\n",
    "RMSE_var, MAPE_var = get_metrics(results, 'VAR')\n",
    "metrics_var= pd.concat([RMSE_var, MAPE_var], axis = 1)\n",
    "metrics_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b52ff",
   "metadata": {},
   "source": [
    "## 3.2 Machine learning models\n",
    "\n",
    "In the second section, we run our machine learning models: ```Ridge Regression (Ridge)```,  ```Least Absolute Shrinkage and Selection Operator (LASSO)``` and ```Random Forest (RF)``` models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea4ca4",
   "metadata": {},
   "source": [
    "### 3.2.1 Ridge Regression (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f008bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_models_regression(models, data, pred_vars, target_var ):\n",
    "#     \"\"\"\n",
    "#     Evalúa modelos de regresión utilizando validación cruzada en series temporales.\n",
    "\n",
    "#     Parámetros:\n",
    "#     - modelos: Diccionario de modelos de regresión para evaluar.\n",
    "#     - datos: DataFrame que contiene el conjunto de datos.\n",
    "#     - variables_predictoras: Lista de nombres de variables predictoras.\n",
    "#     - variable_objetivo: Nombre de la variable objetivo.\n",
    "\n",
    "#     Retorna:\n",
    "#     DataFrame: Resultados de la evaluación del modelo.\n",
    "#     \"\"\"       \n",
    "#     results = {\n",
    "#         'Model': [],\n",
    "#         'R2_train': [],\n",
    "#         'R2_test': [],\n",
    "#         'MAE_train': [],\n",
    "#         'MAE_test': [],\n",
    "#         'MAPE_train': [],\n",
    "#         'MAPE_test': [],\n",
    "#         'MSE_train': [],\n",
    "#         'MSE_test': [],\n",
    "#         'RMSE_train': [],\n",
    "#         'RMSE_test': [],\n",
    "#         'Grid_Search_Params': []\n",
    "#     }\n",
    "    \n",
    "#     X = data[pred_vars]\n",
    "#     y = data[target_var]\n",
    "    \n",
    "#     cv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "#     print(f\"Entrenando y evaluando modelos...\")\n",
    "    \n",
    "#     for model_name, model_params in models.items():\n",
    "#         print(f\"Procesando el modelo: {model_name}\")\n",
    "        \n",
    "#         if 'model' in model_params:\n",
    "#             model = model_params['model']\n",
    "#         else:\n",
    "#             raise ValueError(f'Model is not defined for {model_name}')\n",
    "        \n",
    "#         if 'grid_params' in model_params:\n",
    "#             grid_params = model_params['grid_params']\n",
    "#         else:\n",
    "#             grid_params = None\n",
    "        \n",
    "#         best_params = None\n",
    "        \n",
    "#         for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
    "#             X_train, X_test = X.iloc[tr], X.iloc[tt]\n",
    "#             y_train, y_test = y.iloc[tr], y.iloc[tt]\n",
    "            \n",
    "#             if ii == (cv.n_splits - 1):\n",
    "            \n",
    "#                 if grid_params is not None:\n",
    "#                     grid_search = GridSearchCV(model, grid_params, cv=cv)\n",
    "#                     grid_search.fit(X_train, y_train)\n",
    "#                     best_model = grid_search.best_estimator_\n",
    "#                     best_params = grid_search.best_params_\n",
    "\n",
    "#                     if hasattr( best_model, 'feature_importances_' ):\n",
    "\n",
    "#                         feature_importances = best_model.feature_importances_\n",
    "#                         vars_df             = pd.DataFrame( {'Var': pred_vars, 'Importance Score': feature_importances } )\n",
    "#                         vars_df             = vars_df.reindex( vars_df[ 'Importance Score' ].abs().sort_values( ascending = False ).index )\n",
    "#                         vars_df.to_excel( f'varlist__{ model_name }.xlsx' )\n",
    "\n",
    "#                     elif hasattr( best_model, 'coef_' ):\n",
    "\n",
    "#                         coefficients = best_model.coef_[ 0 ]\n",
    "#                         vars_df      = pd.DataFrame( {'Var': best_model.feature_names_in_, 'Coefficient': coefficients } )\n",
    "#                         vars_df      = vars_df.reindex( vars_df[ 'Coefficient' ].abs().sort_values( ascending = False ).index )\n",
    "#                         vars_df.to_excel( f'varlist_{ model_name }.xlsx' )\n",
    "#                 else:\n",
    "#                     best_model = model.fit(X_train, y_train)\n",
    "#                     coefficients = best_model.coef_[ 0 ]\n",
    "#                     vars_df      = pd.DataFrame( {'Var': best_model.feature_names_in_, 'Coefficient': coefficients } )\n",
    "#                     vars_df      = vars_df.reindex( vars_df[ 'Coefficient' ].abs().sort_values( ascending = False ).index )\n",
    "#                     vars_df.to_excel( f'varlist_{ model_name }.xlsx' )\n",
    "\n",
    "#                 y_pred_train = best_model.predict(X_train)\n",
    "#                 y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "#                 best_model_params = {\n",
    "#                     'Model': model_name,\n",
    "#                     'Grid_Search_Params': best_params\n",
    "#                 }\n",
    "\n",
    "#         results['Model'].append(best_model_params['Model'])\n",
    "#         results['Grid_Search_Params'].append(best_model_params['Grid_Search_Params'])\n",
    "    \n",
    "#     results_df = pd.DataFrame(results)\n",
    "#     results_df = results_df.sort_values(by='RMSE_test', ascending=True)\n",
    "\n",
    "#     return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6c4500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Asumiendo que tu DataFrame tiene una columna 'timestamp' que representa la fecha\n",
    "# # y que 'target_variable' es la variable que deseas predecir\n",
    "\n",
    "# # 1. Filtra los datos hasta diciembre de 2018\n",
    "# train_data = df[df['timestamp'] <= '2018-12-01']\n",
    "\n",
    "# # 2. Filtra los últimos 12 meses para el conjunto de prueba\n",
    "# test_start_date = datetime.strptime('2018-12-01', '%Y-%m-%d') + timedelta(days=1)\n",
    "# test_data = df[(df['timestamp'] >= test_start_date) & (df['timestamp'] <= '2019-12-01')]\n",
    "\n",
    "# # 3. Divide tus datos en características (X) y variable objetivo (y)\n",
    "# X_train = train_data.drop('target_variable', axis=1)  # Ajusta 'target_variable' al nombre de tu variable objetivo\n",
    "# y_train = train_data['target_variable']\n",
    "\n",
    "# X_test = test_data.drop('target_variable', axis=1)\n",
    "# y_test = test_data['target_variable']\n",
    "\n",
    "# # Ahora tienes X_train, y_train para entrenar tu modelo hasta diciembre de 2018,\n",
    "# # y X_test, y_test para evaluar tu modelo en los últimos 12 meses.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
